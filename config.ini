;for multiple vhosts: make sure your logs are written in seperate files with a clear indication of the tld in the filename
;then make one configfile per vhost, and start the logparser with the -config string
[general]
;this is the path where the sqlite database will be stored.
;full path, can be relative to where the logparser is executed, but usually a complete path is better
dbpath = "./demologs/mydb.db"

;format used to parse the timestamp in the logs. More info: https://www.practical-go-lessons.com/post/how-to-format-time-with-golang-ccc5ja83ibmc70m98260
timeformat = "02/Jan/2006:15:04:05 -0700"

;NOT used for splitting vhost logs (see above)!!! just here to identify referrers coming from your own domain!!!
;just a tld, no https, no slashes
mydomain = "merel.mobi"

;write a logs.html file to the output directory?
;true or false (true write the log, false doesn't write te log), no quotes
writelog = true

[input]
;where do i find the logs generated by your webserver?
;full path WITH trailing slash!!! path can be relative to where logparser is executed, but full path is preferred
logfilepath = "./demologs/"

;go regex to select which files to parse. Make sure you skip the logfiles, and the files are either ascii text or gz files (other formats not yet supported)
;tutorial: https://yourbasic.org/golang/regexp-cheat-sheet/
logfileregex = "^merel\.mobi\.acces.*"

;the regex to match your log format. Can either be the actual grouped regex, or a shortcode
;here's an example of an actual regex for a log in combined log format: (?m)^(\S*).*\[(.*)\]\s"(\S*)\s(\S*)\s([^"]*)"\s(\S*)\s(\S*)\s"([^"]*)"\s"([^"]*)"$
;currently only combined log format has a shortcode: "clf"
parseregex = "clf"

;if you enter your own regex with own groups, i need to be able to get the necessary info from the lines i parse. 
;the following config (starting with parserfield_ indicates which group in the parseregex contains which data. 
;the defaults are for clf
parserfield_ip = 1
parserfield_datetime = 2
parserfield_method = 3
parserfield_request = 4
parserfield_httpversion = 5
parserfield_returncode = 6
parserfield_httpsize = 7
parserfield_referrer = 8
parserfield_useragent = 9

[output]
;where do you want me to drop the output files. 
;usually it's a good idear to run the apachelogparser as the owner of the apache process (www-data, apache,...) and put the output somewhere in your web root
;IF you set the "emptyoutputpath", the html files will be cleaned from this directory, but you're safe to put an .htaccess file in there for password protection (or any other type of file or protection for what i care... just don't put html files in the output dir if you want to keep them...)
;full path WITH trailing slash preferred, relative path WITH trailing slash accepted
outputpath = "./output/"

;if set to true, the html files in the outputpath will be removed at the start of a new run, if set to false the script will overwrite the existing files, but won't touch any other html files
emptyoutputpath = true

;some stats only show a subset of days to keep graphs clear and filesizes small. This parameter tells the script in how many days you're interested
;setting this setting to less than 28 days might result in getting less comparative information, setting it to more than 180 might result in illegible graphs and large tables (depending on the amount of logs you have)
number_of_days_detailed = 31

;by default, go-echart's assets are pulled from the web... If you want extra security or run on an intranet, download the assets and serve them locally by changing the go-echarts.github.io path to the intranet path where you serve the assets
assethost = "https://go-echarts.github.io/go-echarts-assets/assets/"

;if you're not dropping the output in a webroot, it might be a good idear to let the script generate an easy to download zipfile containing all the output. This way you can easily fetch the zip to your local system, unpack it, and view it as a local file (open the index.html)
;options are true (create a zip) or false (don't create a zip)
zipoutput = false

;if zipoutput = true, this parameter tells the script where exactly it needs to put the zipfile AND it's name
;a full path, including the actual zip filename is preferred, a relative path is allowed
zippath = ./output.zip

;the stat about referrers shows a maximum number of referrers only as to keep the resulting table relative small... If you want to see more referrers, just increase this number... It's all ascii anyways, even a large number won't hurt you that much ;)
numberofreferrers = 30

[ignorevisitorips]
;visitors having these ip's won't be loaded into the database. Use this to blacklist your own ips, or the ip's from hackers or bots you do not want to include in your statistics. Once they're ignored and the apachelogparser is run in logparsing mode, it's pretty hard to get ignored visits back into the database!
;the format is pretty simple: free_text_reminder_for_you = "the ip you want to ignore"
;each line is a new ip you want to ignore, the key value is just free text, avoid special characters and spaces tough!
myownip = "127.0.0.1"

[ignorehostagents]
;visitors having these host agents won't be loaded into the database. Use this to blacklist search engine crawlers, spiders, bots or the host agents from hackers or bots you do not want to include in your statistics. Once they're ignored and the apachelogparser is run in logparsing mode, it's pretty hard to get ignored visits back into the database!
;the format is pretty simple: free_text_reminder_for_you = "regex of the host agent you want to ignore"
;each line is a new host agent you want to ignore, the key value is just free text, avoid special characters and spaces tough!
google = "(?i)google"
bing = "(?i)bing"
yandex = "(?i)yandex"
bots = "(?i)bot"
wordpress = "(?i)wordpress"
wget = "(?i)wget"
spider = "(?i)spider"
java = "(?i)java"

[ignorereferrers]
;visitors having these referrers won't be loaded into the database. Use this to blacklist any referrer you're not interested in. Once they're ignored and the apachelogparser is run in logparsing mode, it's pretty hard to get ignored visits back into the database!
;the format is pretty simple: free_text_reminder_for_you = "regex of the referrer you want to ignore"
;each line is a new referrer you want to ignore, the key value is just free text, avoid special characters and spaces tough!
me = "(?i)localhost"

[ignoredrequests]
;visitors having these requests won't be loaded into the database. Use this to blacklist any request you're not interested in, like robots.txt, images, movies, css, javascript, requests coming from your wordpress admin pages,.... Once they're ignored and the apachelogparser is run in logparsing mode, it's pretty hard to get ignored visits back into the database!
;the format is pretty simple: free_text_reminder_for_you = "regex of the request you want to ignore"
;each line is a new request you want to ignore, the key value is just free text, avoid special characters and spaces tough!
robots = ".*robots\.txt$"
img_png = "\.png$"
img_jpg = "\.jpg$"
img_gif = "\.gif$"
json = "\.json$" 
java = "\js$"
css = "\.css$"
wp = "wp-"